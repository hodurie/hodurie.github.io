---
title: "Logistic(Regression) Classification 02"
type: post
permalink: /study/ai/ML_Lec_05_01
category: 
    - STUDY
        - etc
tag:
    - AI
use_math: true
toc: ture
siderbar_main: true
---
본 글은 [모두를 위한 머신러닝/딥러닝 강의](https://hunkim.github.io/ml/)를 참고하여 작성하였습니다.  

# ML lec 5-2 Logistic Regression의 cost 함수 설명
## Linear Cost Function
먼저, 이전 글에서 설명한  
로지스틱 회귀의 가설은  

$$ H(X) = \frac{1}{1+e^{-W^TX} } $$ 

이다.  

이는 $y$의 값을 0과 1사이의 값으로 변환 시켜주는 식이다.  

이때, 우리가 $W$를 $x$축에 두고,  
선형 회귀식의 $Cost(W)$을 $y$축에 두면  
약과의 겉 테두리 부분처럼  
각 지점에 굴곡이 있는 아래로 볼록한 그래프가 만들어진다.  

이를 이용해서 Gradient Descent 기법을 사용하게되면,  
시작점이 어디냐에 따라 최적의 값이 달라지게 된다.  

이를 방지하고자,  
우리는 비용함수를 새롭게 정의해야 한다.  

## Logistic Cost Function
$$ \mathbf{cost}(W) = \frac{1}{m}\sum \mathbf{c}(H(x), y)$$

$$ \text{c}(H(x),y) = 
\begin{cases}
-log(H(x)) & y = 1\\
-log(1-H(x)) & y = 0
\end{cases} $$

첫 번째로,  
가설에 사용된 $e$에 대응되는 값이  
$log$ 값이라는 점

두 번째로,  
$g(z) = -log(z)$가 있다고 가정할 시,  
$z$의 값이 거의 동일하면 $g(z)$는 0의 가까운 값을 갖고  
$z$의 값이 일치하지 않으면 $g(z)$는 무한대에 가까운 값은 갖는 점

## Cost Function 재표현  
위와 같은 식으로 표현했을 때,  
식이 복잡하게 느껴질 수 있다.  

이를 간편하게 하나의 식으로 줄일 수 있다.  

$$ 
\text{c}(H(x),y) = 
-ylog(H(x)) - (1-y)log(1-H(x)) $$

$y$의 값에 1을 넣게 되면,  
뒤의 식이 사라져  
$-log(H(x))$의 식만 살아나고  

$y$의 값에 0을 넣게 되면,  
앞의 식이 사라져  
$-log(1- H(x))$의 식만 살아난다. 


