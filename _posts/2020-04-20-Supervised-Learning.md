---
title: "지도학습"
type: post
permalink: /study/ai/sl
category: 
    - STUDY
        - etc
tag:
    - AI
toc: ture
siderbar_main: true
---
본 내용은 [인공지능의 이해](https://www.edwith.org/knusw-ai)의 내용을 참조하여 작성하였습니다.  

# 지도 학습
오늘은 머신러닝의 학습 방법 중 지도학습에 대해 알아보겠습니다.
지도 학습은 데이터의 입출력 관계를 파악하여 값을 예측하는 방법입니다.  
모델들을 학습시켜 출력값을 예측하는 데,  
이때, 우리는 데이터를 어떻게 학습시켜야 할까요?

## 데이터 학습
먼저, 데이터를 학습 데이터와 테스트 데이터로 나눕니다.  
나눠진 학습 데이터 분류기를 통해 입출력 대응 관계를 파악합니다.  
이후, 모델이 잘 예측 또는 분류 하고 있는지를 분류기를 평가하여 확인해봅니다.  
분류기의 성능이 괜찮다면, 테스트 데이터에 분류기를 적합하여 값을 예측합니다.  

다음은 지도 학습의 종류에 대해 자세히 알아보도록 하겠습니다.  
지도 학습은 크게 **분류**와 **회귀**로 나뉩니다.  

## 분류
학습 데이터의 입/출력 대응 관게를 만족시키는 함수 또는 규칙을 구하여,  
입력 데이터를 특정한 항목들로 나누는 방법입니다,  

여기서 우리는 학습 데이터를 잘 분류할 수 있는 함수를 찾는 것이 가장 중요합니다.  
함수는 수학적 함수일수도 있고 규칙일 수도 있습니다.

이때, 우리는 프로그램을 이용하여 학습된 함수를 분류하게 되는데 이를 **분류기**라고 합니다.  
분류기를 이요안 학습 알고리즘은 다음과 같습니다.  
- 결정트리 알고리즘
- K-근접이웃 알고리즘
- 다층 퍼셉트론 신경망
- 딥러닝 알고리즘
- 서포트 벡터 머신
- 에이다부스트
- 랜덤 포레스트
- 확률 그래프 모델

### 결정트리
데이터를 이진트리를 이용하여 규칙에 따라 구분하는 알고리즘입니다.  
비교 속성인 **내부 노드**와 속성 값을 나타내는 **간선**, 대표값 또는 분류 값인 **단말 노드** 로 구성돼 있습니다.  

그렇다면, 우리는 어떤 규칙을 기반으로 트리를 구성해 나가야 할까요?
결정트리는 **엔트로피** 또는 **정보이득**을 기반으로 가지를 뻗어나갈지를 결정합니다.  

**엔트로피**  
엔트로피는 대상이 얼마나 동일한지 아닌지를 판단하는 척도입니다.  
값이 높으면 높을수록 데이터들이 동일한 값으로 구성 돼 있다는 의미며,  
낮으면 낮을 수록 데이터들이 다른 값들과 섞여 있다는 의미입니다.  

**정보이득**
정보이득은 분할 전 앤트로피 값이 특정 속성으로 분할 후 얼마만큼 변화하는지를 나타낸 척도입니다.  
이때, 정보이득이 가장 큰 속성으로 트리의 분할을 실시합니다.  

하지만, 속성 값이 많은 데이터를 정보이득 척도를 이용하여 분할하면 너무 많은 부분집합으로 나눠지게 되는 경향이 있습니다.  

이를 보안하기 위해 정보이득비의 개념을 도입했습니다.  
정보이득비는 그 지점의 엔트로피로 나눠 나타낸 값으로,  
과도한 정보이득을 갖는 속성 값을 약화 시킵니다.  

### K-근접이웃 알고리즘
기존의 데이터의 분류값을 이용해 구하는 알고리즘으로,  
새로운 데이터가 들어 왔을 때 주변 K개의 데이터가 어떤 분류 값을 갖는지를 파악하고,  
다수의 범주 값으로 새로운 데이터의 값을 할당하는 기법으로,  
주변 분류값만 알면 구할 수 있는 간단한 알고리즘이다.  

하지만, 기존 데이터의 값을 이용해 새로운 데이터를 분류하므로,  
메모리의 사용이 큰 편이다.

### 서포트 벡터 머신
데이터들을 구분할 수 있는 최적의 경계선을 찾는 기법으로,  
경계선은 원 데이터보다 한 단계 낮은 차원으로 구성 된다.  
이때, 차원의 경계선은 분류된 값들의 거리를 최대로 하는 방법으로 생성된다.

### 앙상블
앙상블은 다수의 모델을 조합하여 하나의 모델을 만드는 기법을 말합니다.

대표적인 예로는  
**배깅**, **스태킹**, **부스팅**이 있습니다.

**배깅**  
분류기가 투표를 통해 최종 예측 결과를 결정하며,  
**bootstrapping**이라는 기법을 사용한다.
> bootstrapping은 중복을 허용하는 단순 임의 추출

**알고리즘 작동 방법**  
학습 데이터를 bootstrapping을 이용하여 데이터를 추출하고,  
각각의 모델에 적합후 라벨 값은 voting을,  
수치 데이터는 평균을 이용하여 최종값을 예측합니다.

여기서, voting은 두 가지로 나눠집니다.
- Hard voting : 다수결의 원칙
- Soft voting : 값들의 확률값을 고려

**스태킹**  
스태킹은 하나의 데이터를 여러 subset으로 나눠  
하나의 subset에 분류기를 학습시킨 후,  
다른 subset에 적용해 에측한 값들을 섞어 새로운 분류기를 만드는 것을 말한다.

**부스팅**  
weak learner로 잘못 분류된 값에 가중치 둬  
잘못 예측한 값을 잘 예측하도록 만드는 기업입니다.  

지금까지 지도학습 중 분류 알고리즘에 대해 알아보았습니다.
다음은 회귀 알고리즘에 대해 알아보도록 하곘습니다.

## 회귀
분류의 값들은 출려값이 유한한 개수의 라벨 값이 였다면,  
회귀의 값들은 출력값이 실수인 값입니다.  
여기서 회귀의 목표는 학습 데이터의 패턴 및 함수를 파악하여 실수 값으로 예측하는 것 입니다.

우리는 함수를 파악하여 값을 예측하게 되는데,  
이때 우리는 오차의 개념을 도입합니다.  

오차란 예측값과 실제값(관측치)의 차이를 말합니다.

### 로지스틱 회귀
로지스틱 회귀의 경우 회귀이나 이진법적인 선택이 필요한 경우 사용합니다.