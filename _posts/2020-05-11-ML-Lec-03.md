---
title: "Linear Regression cost 함수 최소화"
type: post
permalink: /study/ai/ML_Lec_03
category: 
    - STUDY
        - etc
tag:
    - AI
use_math: true
toc: ture
siderbar_main: true
---
본 글은 [모두를 위한 머신러닝/딥러닝 강의](https://hunkim.github.io/ml/)를 참고하여 작성하였습니다.

# ML lec 03 - Linear Regression의 cost 최소화 알고리즘의 원리 설명
## Hypothesis and Cost
선형 회귀에서의 가설과 손실 함수는  

$$
H(x) = Wx + b
$$

$$\mathbf{cost}(W, b) = { {1} \over {m} } \sum_{i = 1}^m (H(x_i) - y_i)^2 $$


입니다.  

여기서 손쉬운 설명을 위해 $b$를 제외하고 설명하도록 하겠습니다.  
그렇다면 식은,

$$
H(x) = Wx
$$

$$
\mathbf{cost}(W) = { {1} \over {m} } \sum_{i = 1}^m (H(x_i) - y_i)^2 $$

$$  

이 됩니다.

위 식을 갖고 아래와 같은 데이터에 적용한다고 생각해 봅시다.  

x|y
---|---
-2|-2
-1|-1
0|0
1|1
2|2

$W = 0$ 일 때, $\mathbf{cost}(W) = 2$  
$W = 1$ 일 때, $\mathbf{cost}(W) = 0$  
$W = 2$ 일 때, $\mathbf{cost}(W) = 2$  
이를 무수히 반복한다고 할 때,  
$W$를 $x$축에 $\mathbf{cost}(W)$ 값을 $y$ 축에 그린다면,  
우리는 아래로 볼록한 그래프가 그려질 것입니다.  

이때, $\mathbf{cost}(W)$를 최소화 하는 $W$ 값을 어떻게 찾아야 할까요?

## Gradient Descent Algorithm
**Gradient Descent Algorithm**는 경사 하강 알고리즘으로,    
비용 함수(손실 함수)를 최소화 시키는 데 사용하는 알고리즘입니다.  

작동 원리는  
임의의 점에서 시작하여,  
먼저, $W$와 $b$의 값을 조금씩 수정해 보면서 $\mathbf{cost}(W, b)$의 변화를 살펴보고,  
그 중 $\mathbf{cost}(W, b)$값을 가장 최소화 시킬 수 있는 경사를 선택해 $\mathbf{cost}(W, b)$를 줄여갑니다.  
이 과정을 최소값을 얻었다고 생각할 때 까지 계속해 나갑니다.  

여기서, 경사라는 단어가 나왔습니다.  
그렇다면, 우리는 이 경사도를 어떻게 구해서 $\mathbf{cost}(W, b)$를 줄여나갈까요?

정답은, 미분을 이용합니다.  
먼저, 이를 설명하기 위해 앞서 언급한 $b$가 제거된 식을 사용하고,  
약간의 식의 변형을 주겠습니다.  

첫 번째로, $\mathbf{cost}(W)$ 식을 살짝 변형 시키겠습니다.  

$$\mathbf{cost}(W) = { {1} \over {m} } \sum_{i = 1}^m (Wx_i - y_i)^2 $$

위와 같은 식에서  
나누는 값을 $2m$으로 변형 시킵니다.  
이는 미분의 계산을 용이하게 하기 위합입니다.  

이때, $m$에서 $2m$으로 변경하여도 $\mathbf{cost(W)}$ 자체에는 영향을 주지 않습니다.  

두 번째는, $W$를 변형 시키겠습니다.  

$$W := W - \alpha{ {\partial} \over {\partial{W} } } \mathbf{cost}(W)$$

여기서 $\alpha$는 Learning Rate로 학습률로 임의의 상수 입니다.  
$W$의 식을 간단히 설명하자면,  
임의의 점에서 $W$의 미분값인 기울기를 구하고  
기울기의 값이 음수 값일 땐 $W$를 조금 높게 설정하고  
기울기의 값이 양수 값일 땐 $W$를 조금 낮게 설정하자  
이렇게 생각하시면 됩니다.  

본격적으로 미분을 해봅시다.  
먼저, $W$ 식에 $\mathbf{cost}(W)$에 이전에 설정한 식을 대입합니다.  

$$W := W - \alpha{ {\partial} \over {\partial{W} } }
 { {1} \over {m} } \sum_{i = 1}^m (Wx_i - y_i)^2$$

위의 식을 미분합니다.  

$$W := W - \alpha{ {1} \over {m} } \sum_{i = 1}^{m}2 (Wx_i - y_i)x_i$$

식을 다시 정리해 주면  

$$W := W - \alpha{ {1} \over {m} } \sum_{i = 1}^m(Wx_i - y_i)x_i$$

위 식을 갖고 컴퓨터가 이해할 수 있게만 표현시켜준다면,  
우리는 직접 미분을 계산하지 않고 최적의 $W$의 값을 찾을 수 있습니다. 