---
title: "Logistic(Regression) Classification 01"
type: post
permalink: /study/ai/ML_Lec_05_02
category: 
    - STUDY
        - etc
tag:
    - AI
use_math: true
toc: ture
siderbar_main: true
---
본 글은 [모두를 위한 머신러닝/딥러닝 강의](https://hunkim.github.io/ml/)를 참고하여 작성하였습니다.  

# ML lec 5-1: Logistic Classification의 가설 함수 정의
## Regression
Regression에서는 우리는 세 가지 가정을 한다.  
첫 번째, 가정이다.  
- 이 데이터는 선형적인 관계가 존재할 것인가?
- 다변수를 가지지고 있는 선형회귀 식이 존재할 것이다.

두 번째, Cost Function  
주어진 가설을 바탕으로,  
각각의 비용값들 계산  

세 번째, Gradient Decent  
위의 식을 바탕으로,  
값을 최소화 시키는 $W$ 찾기  

## Classification
데이터를 특정 기준에 따라 분류하는 것  

사용예,  
- 스팸메일 분류(일반 메일 또는 스팸 메일)
- 시험 합격 여부(합격 또는 불합격)
- 암 종양 여부(음성 또는 양성)

이 장에서 다룰 내용은   
0과 1의 값을 갖는    
**Binary Classification**이다.  

이때, 우리가 값을 분류하기 위해  
선형 식을 이용한다고 해보자.  
  
우리가 생각 할 수 있는 최선의 방법은  
$y$ 값이 0.5 되는 지점과  
기울기가 만나는 지점을 기점으로  
$x$ 값을 나누는 것이다.  

하지만, 여기서 문제가 발생한다.  
기울기에 따라 0의 값이 1의 값으로 분류 될 수 있고,  
반대의 경우도 생길 수 있다.  

이런 문제점을 보완하고자 나온것이  
**Logistic Regression** 이다.  

## Logistic Hypothesis
$$ H(X) = \frac{1}{1 + e^{-W^TX} } $$