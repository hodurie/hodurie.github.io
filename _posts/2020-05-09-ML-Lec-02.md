---
title: "Linear Regression의 개념"
type: post
permalink: /study/ai/ML_Lec_02
category: 
    - STUDY
        - etc
tag:
    - AI
use_math: true
toc: ture
siderbar_main: true
---

본 글은 [모두를 위한 머신러닝/딥러닝 강의](https://hunkim.github.io/ml/)를 참고하여 작성하였습니다.

# ML lec 02 - Linear Regression의 Hypothesis와 cost 설명
## Regression
**Regression**은 정답 값이 있는 지도 학습의 한 종류로,  
여러 변수들의 관계들을 바탕으로 정답 값을 예측하는 것을 말합니다.  

오늘은 Regression 중 Linear Regression,  
선형 회귀에 대해 알아보겠습니다.  

## Linear Regression
선형 회귀란,  
정답 값과 변수들 간의 관계가 선형적인 구조를 띠고 있는 것을 말합니다.  
하지만, 모든 데이터가 선형적인 관계를 갖고있는지는 미지수 입니다.  
이를 검증하기 위해서 우리는 가설을 세웁니다.  

선형 회귀의 경우 일반적인 가설을 이렇습니다.  

$$ H(x) = Wx + b $$  

가설을 세웠지만, 이에 해당하는 식은 무수히 많을 것 입니다.  
우리는 이 식들 중 데이터들을 잘 설명할 수 있는 하나의 식을 찾아줘야 합니다.  
이를 확인하기 위해 사용하는 함수가 **Cost Function** 입니다.  

## Cost Function
**Cost Function** 이란,  
실제 값과 식을 통해 구한 값이 얼마나 차이가 나는지를 확인해주는 함수로,  
회귀 식에서는 **Loss Function** 이라고도 합니다.  

구하는 식은  

$$ H(x) - y $$  

이와 같이 표현할 수 있지만,  
차이가 많이 나는 값들에 대해선 패널티를 줄 수 있고,  
모든 값들을 양수로서 표현해 비교하기 쉽게 만들기 위해  

$$ \left(H(x) - y\right)^2 $$  

이와 같이 나타냅니다.  

좀 더 일반적인 표현으로 나타낸다면,  

$$
\mathbf{cost} = 
{ {1}\over{m} }
\sum_{i=1}^m(H(x_i) - y_i)^2
$$

여기서,  
$m$ 은 데이터의 개수  
$i$ 는 데이터의 위치로 보면 됩니다.  

예를들어,  
아래와 같은 데이터가 있다고 가정하면,  

idx|x|y
---|---|---
1|1|2
2|2|4
3|3|6

데이터의 개수는 세 개. 즉, $m$ = 3  
$x$ = 2 인 데이터는, 두 번째 데이터. 즉 $i$ = 2   

다시 돌아와  
위의 식에 대해 설명하면,  
$ H(x)$는 $ H(x) = Wx + b $와 같은 식으로,  
$W$와 $b$로 구성된 식입니다.  

즉, 위의 식을  

$$ 
\mathbf{cost}(W, b) = { {1}\over{m} } \sum_{i = 1}^m(H(x_i) - y_i)^2 
$$

와 같이 작성할 수 있습니다.  

다시 돌아와 우리가 이 손실 함수를 구하는 목적을 다시 되짚어 봅시다.  
우리는 데이터들이 선형적인 관계가 존재할 것이다 라고 가정을 했고,    
이를 만족시키는 식을 찾고 싶었던 겁니다.  
그런데, 아무식이나 막 쓸 수 없으니,  
실제값과 우리가 예측해서 구한 값들 간 차이가 거의 없는 식을 찾자!  
즉, `Cost Function`이 최소가 되는 식을 찾는 것이 우리의 목적입니다.  

이를 식으로서 나타낸다면,  

$$
\mathbf{minimize}_{W, b} \mathbf{cost}(W, b)
$$

이와 같이 나타낼 수 있습니다.